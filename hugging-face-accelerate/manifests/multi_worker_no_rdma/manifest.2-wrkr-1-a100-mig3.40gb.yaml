# This manifest run the training with:
# - 2 container
# - 1 NVIDIA-A100-SXM4-80GB-MIG-3g.40gb GPU per container (2*1=4GPUs)
#
# - mixed_precision fp16
# - batch_size 48
# - epochs 12
# - model: tinyllama
# Execution time: 21m10s.

kind: AIchorManifest
apiVersion: 0.2.2

builder:
  image: image
  context: hugging-face-accelerate # hugging-face-accelerate folder
  dockerfile: ./Dockerfile

spec:
  operator: pytorch
  image: image
  command: "torchrun --nproc_per_node 1 main.py --mixed_precision fp16 --batch_size=48" # --nproc_per_node=={Number of GPUs}

  tensorboard:
    enabled: true

  types:
    Master:
      count: 1
      resources:
        cpus: 2
        ramRatio: 16 # 32GB
        accelerators: # optional
          gpu:
            count: 1 # note that setting more than 1 MIG device per container is useless since only one will be usable https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#cuda-device-enumeration
            type: gpu
            product: NVIDIA-A100-SXM4-80GB-MIG-3g.40gb
    Worker:
      count: 1
      resources:
        cpus: 2
        ramRatio: 16 # 32GB
        accelerators: # optional
          gpu:
            count: 1 # note that setting more than 1 MIG device per container is useless since only one will be usable https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#cuda-device-enumeration
            type: gpu
            product: NVIDIA-A100-SXM4-80GB-MIG-3g.40gb

