# This manifest run the training with:
# - 4 Workers (technically 1 Master + 3 Workers)
# - 2 NVIDIA-H100-80GB-HBM3 GPU per container (4*2=8GPUs)
#
# - mixed_precision fp16
# - batch_size 48
# - epochs 48
# - model: tinyllama
# Execution time: 3m45s

kind: AIchorManifest
apiVersion: 0.2.2

builder:
  image: image
  context: hugging-face-accelerate # hugging-face-accelerate folder
  dockerfile: ./Dockerfile

spec:
  operator: pytorch
  image: image
  command: "torchrun --nproc_per_node 2 main.py --mixed_precision bf16 --batch_size 112 --num_epoch 48" # --nproc_per_node=={Number of GPUs per worker}

  tensorboard:
    enabled: true
  
  # debug:
  #   vscode:
  #     enabled: false
  #     path: "/aichor/code"
  #     provider: "github"

  types:
    Master:
      count: 1
      resources:
        cpus: 4
        ramRatio: 16 # 64GB
        rdma:
          # list of network devices to mount on the container
          # In this example running on InstaDeep's Kyber cluster, GPU nodes have 4 PF (Physical Function, NIC) each.
          # Request VF (Virtual Function) from each one of them and let NCCL find the
          # fastest, most optimized routes for GPU to GPU communication
          devices: ["sriov_a", "sriov_b", "sriov_c", "sriov_d"]
        accelerators: # optional
          gpu:
            count: 2
            type: gpu
            product: NVIDIA-H100-80GB-HBM3
    Worker:
      count: 3
      resources:
        cpus: 4
        ramRatio: 16 # 64GB
        rdma:
          # list of network devices to mount on the container
          # In this example running on InstaDeep's Kyber cluster, GPU nodes have 4 PF (Physical Function, NIC) each.
          # Request VF (Virtual Function) from each one of them and let NCCL find the
          # fastest, most optimized routes for GPU to GPU communication
          devices: ["sriov_a", "sriov_b", "sriov_c", "sriov_d"]
        accelerators: # optional
          gpu:
            count: 2
            type: gpu
            product: NVIDIA-H100-80GB-HBM3
